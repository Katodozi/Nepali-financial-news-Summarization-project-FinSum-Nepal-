{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3011a538-8209-41c7-b5db-3c4ff63e25c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8639 tokenized sentences.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the tokenized data from CSV file\n",
    "df = pd.read_csv(r\"C:\\Users\\Anuz\\OneDrive\\Desktop\\excel work\\stemmed_tokenized_cleaned_dataset.csv\")\n",
    "\n",
    "# Assuming your tokenized sentences are in a column named 'Concatenated Paragraphs'\n",
    "tokenized_sentences = df['stemmed_text'].apply(eval).tolist()  # Convert string representation to list\n",
    "\n",
    "# Verify loaded sentences\n",
    "print(f\"Loaded {len(tokenized_sentences)} tokenized sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d137a0-c146-45a3-a641-a69be4efb94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GloVe:\n",
    "    def __init__(self, vocab_size, embedding_dim, window_size, learning_rate=0.05):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.word_embeddings = np.random.rand(vocab_size, embedding_dim) * 0.01  # Initialize word embeddings\n",
    "        self.co_occurrence = np.zeros((vocab_size, vocab_size))\n",
    "\n",
    "    def build_co_occurrence_matrix(self, tokenized_sentences):\n",
    "        for sentence in tokenized_sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                word_index = self.word_to_index(word)\n",
    "                start = max(0, i - self.window_size)\n",
    "                end = min(len(sentence), i + self.window_size + 1)\n",
    "                for j in range(start, end):\n",
    "                    if i != j:\n",
    "                        context_word_index = self.word_to_index(sentence[j])\n",
    "                        self.co_occurrence[word_index][context_word_index] += 1\n",
    "\n",
    "    def word_to_index(self, word):\n",
    "        # Map word to index (this should depend on how you handle vocab)\n",
    "        return hash(word) % self.vocab_size  # Placeholder for actual mapping based on vocab\n",
    "\n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(self.vocab_size):\n",
    "                for j in range(self.vocab_size):\n",
    "                    if self.co_occurrence[i][j] > 0:\n",
    "                        # Calculate loss and update embeddings with numerical stability\n",
    "                        dot_product = np.dot(self.word_embeddings[i], self.word_embeddings[j])\n",
    "                        loss = self.co_occurrence[i][j] - dot_product\n",
    "                        \n",
    "                        # Update embeddings with clipping to avoid overflow\n",
    "                        update_i = self.learning_rate * loss * self.word_embeddings[j]\n",
    "                        update_j = self.learning_rate * loss * self.word_embeddings[i]\n",
    "\n",
    "                        # Clipping updates to prevent overflow\n",
    "                        update_i = np.clip(update_i, -1.0, 1.0)\n",
    "                        update_j = np.clip(update_j, -1.0, 1.0)\n",
    "\n",
    "                        self.word_embeddings[i] += update_i\n",
    "                        self.word_embeddings[j] += update_j\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}: Training GloVe model...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ad9d32-d6c4-436a-9f29-5b410fe1c774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 43047\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = set()\n",
    "for tokens in tokenized_sentences:\n",
    "    # Assuming tokens are stored as space-separated strings; adjust if necessary\n",
    "    unique_tokens.update(tokens)  # Split by spaces to get individual tokens\n",
    "\n",
    "vocab_size = len(unique_tokens)\n",
    "print(f\"Vocabulary Size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a4ac791-c107-4864-bb0d-0ff5fbc1e2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training GloVe model...\n",
      "Epoch 10: Training GloVe model...\n",
      "Epoch 20: Training GloVe model...\n",
      "Epoch 30: Training GloVe model...\n",
      "Epoch 40: Training GloVe model...\n",
      "Epoch 50: Training GloVe model...\n",
      "Epoch 60: Training GloVe model...\n",
      "Epoch 70: Training GloVe model...\n",
      "Epoch 80: Training GloVe model...\n",
      "Epoch 90: Training GloVe model...\n",
      "GloVe training completed.\n"
     ]
    }
   ],
   "source": [
    "# Example parameters (adjust these according to your dataset)\n",
    "vocab_size = 43047  # Adjust this based on your actual vocabulary size\n",
    "embedding_dim = 100\n",
    "window_size = 5\n",
    "\n",
    "# Initialize GloVe model\n",
    "glove_model = GloVe(vocab_size=vocab_size, embedding_dim=embedding_dim, window_size=window_size)\n",
    "\n",
    "# Build co-occurrence matrix\n",
    "glove_model.build_co_occurrence_matrix(tokenized_sentences)\n",
    "\n",
    "# Train GloVe model\n",
    "glove_model.train(epochs=100)\n",
    "\n",
    "print(\"GloVe training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "641d2fc3-ba9b-4c82-ba7f-d14db02c4a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to C:\\Users\\Anuz\\OneDrive\\Desktop\\excel work\\Embeddings.txt\n"
     ]
    }
   ],
   "source": [
    "# Save the embeddings after training\n",
    "output_file_path = r\"C:\\Users\\Anuz\\OneDrive\\Desktop\\excel work\\Embeddings.txt\"  # Update with your desired file path\n",
    "\n",
    "# Create reverse vocabulary mapping\n",
    "word_to_index = {word: idx for idx, word in enumerate(set(word for sentence in tokenized_sentences for word in sentence))}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(glove_model.word_embeddings)):  # Use the length of the embeddings\n",
    "        word = index_to_word[i]  # Get the word corresponding to the index\n",
    "        embedding_vector = ' '.join(map(str, glove_model.word_embeddings[i]))  # Convert vector to string\n",
    "        f.write(f\"{word} {embedding_vector}\\n\")  # Write word and its corresponding embedding\n",
    "\n",
    "print(\"Embeddings saved to\", output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5bac21-d900-4a7d-9e66-3f43b90c5bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
