{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37ddc0d-a284-4204-9f30-c9c9afa52a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming and lemmatization\n",
    "#we tried using the pretrained stemming and lemmatization approach but the custom module which was found\n",
    "#in the github was not compatable and up to date so we are building our own custom stemming funtion \n",
    "#and implement it our tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293a15a3-41a9-455d-8a91-0f9f89ab3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d51d5afc-0d80-4013-8d61-ddffaaf2101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/Anuz/OneDrive/Desktop/excel work/Tokenized_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c471e4c-8b7a-4877-96c3-7a06c2783027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokenized Dataframe:\n",
      "                             Concatenated Paragraphs  \\\n",
      "0  काठमाडौँ । ग्लोबल आइएमई बैंक लिमिटेड महालक्ष्म...   \n",
      "1  काठमाडौँ l सिद्धार्थ बैंक लिमिटेडले प्रधानमन्त...   \n",
      "2  काठमाडौं l नेपालको बैंक नेपाल बैंक लिमिटेड समा...   \n",
      "3  हालका वर्षहरूमा नेपालमा अनलाइन वित्तीय ठगीका घ...   \n",
      "4  रतन टाटाले डुबेका कम्पनी किन्ने साहसिक निर्णय ...   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0  ['काठमाडौँ', '।', 'ग्लोबल', 'आइएमई', 'बैंक', '...  \n",
      "1  ['काठमाडौँ', 'l', 'सिद्धार्थ', 'बैंक', 'लिमिटे...  \n",
      "2  ['काठमाडौं', 'l', 'नेपालको', 'बैंक', 'नेपाल', ...  \n",
      "3  ['हालका', 'वर्षहरूमा', 'नेपालमा', 'अनलाइन', 'व...  \n",
      "4  ['रतन', 'टाटाले', 'डुबेका', 'कम्पनी', 'किन्ने'...  \n"
     ]
    }
   ],
   "source": [
    "print(\"Original Tokenized Dataframe:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caa9642e-c5fb-4cd8-ab28-2b96db8d2e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe after removing punctuation:\n",
      "                                      tokenized_text  \\\n",
      "0  ['काठमाडौँ', '।', 'ग्लोबल', 'आइएमई', 'बैंक', '...   \n",
      "1  ['काठमाडौँ', 'l', 'सिद्धार्थ', 'बैंक', 'लिमिटे...   \n",
      "2  ['काठमाडौं', 'l', 'नेपालको', 'बैंक', 'नेपाल', ...   \n",
      "3  ['हालका', 'वर्षहरूमा', 'नेपालमा', 'अनलाइन', 'व...   \n",
      "4  ['रतन', 'टाटाले', 'डुबेका', 'कम्पनी', 'किन्ने'...   \n",
      "\n",
      "                              cleaned_tokenized_text  \n",
      "0  [काठमाडौँ, ग्लोबल, आइएमई, बैंक, लिमिटेड, महालक...  \n",
      "1  [काठमाडौँ, सिद्धार्थ, बैंक, लिमिटेडले, प्रधानम...  \n",
      "2  [काठमाडौं, नेपालको, बैंक, नेपाल, बैंक, लिमिटेड...  \n",
      "3  [हालका, वर्षहरूमा, नेपालमा, अनलाइन, वित्तीय, ठ...  \n",
      "4  [रतन, टाटाले, डुबेका, कम्पनी, किन्ने, साहसिक, ...  \n"
     ]
    }
   ],
   "source": [
    "#as we can see some of the punctuation are still there even after removing them so let's remove them manually \n",
    "import string\n",
    "#function to remove punctuations \n",
    "def remove_punctuation(tokens):\n",
    "    punctuations = ['।', '!', '?', ',', '.', '(', ')', '[', ']', '{', '}', ':', ';', 'l',  '“', '”', '‘', '’','छ।' ]\n",
    "    return [word for word in tokens if word not in punctuations]\n",
    "\n",
    "#applying the funtion in the tokenized text \n",
    "df['cleaned_tokenized_text'] = df['tokenized_text'].apply(lambda x: remove_punctuation(eval(x)))\n",
    "#here eval function is used to convert the string to list since the removal of punctuation requires it\n",
    "print(\"Dataframe after removing punctuation:\")\n",
    "print(df[['tokenized_text', 'cleaned_tokenized_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8bac982-106e-424b-8834-551fbafd1633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe after stemming:\n",
      "                              cleaned_tokenized_text  \\\n",
      "0  [काठमाडौँ, ग्लोबल, आइएमई, बैंक, लिमिटेड, महालक...   \n",
      "1  [काठमाडौँ, सिद्धार्थ, बैंक, लिमिटेडले, प्रधानम...   \n",
      "2  [काठमाडौं, नेपालको, बैंक, नेपाल, बैंक, लिमिटेड...   \n",
      "3  [हालका, वर्षहरूमा, नेपालमा, अनलाइन, वित्तीय, ठ...   \n",
      "4  [रतन, टाटाले, डुबेका, कम्पनी, किन्ने, साहसिक, ...   \n",
      "\n",
      "                                        stemmed_text  \n",
      "0  [काठमाडौँ, ग्लोबल, आइएमई, बैंक, लिमिटेड, महालक...  \n",
      "1  [काठमाडौँ, सिद्धार्थ, बैंक, लिमिटेड, प्रधानमन्...  \n",
      "2  [काठमाडौं, नेपाल, बैंक, नेपाल, बैंक, लिमिटेड, ...  \n",
      "3  [हाल, वर्षहरू, नेपाल, अनलाइन, वित्तीय, ठगी, घट...  \n",
      "4  [रतन, टाटा, डुबे, कम्पनी, किन्ने, साहसिक, निर्...  \n"
     ]
    }
   ],
   "source": [
    "#defining a custom stemming function \n",
    "import ast \n",
    "def custom_stem(word): \n",
    "    suffixes = [\n",
    "        \n",
    "     'अर्को', 'बाट', 'बाहेक', 'बाहिर', 'बाहिरपट्टी',\n",
    "    'भित्र', 'का', 'करिब', 'को', 'छ', 'छिन्',\n",
    "    'जोड', 'ले', 'लागि',\n",
    "    'लाई', 'माथि', 'मन्तिर', 'मा', 'नजिक',\n",
    "    'पछाडि', 'पहिला', 'पारि', 'प्रति', 'र',\n",
    "    'संग','सहित','तल','तर','तिर',\n",
    "    'तर्फ','उपर','विपरित','वरिपरि','भित्र','बिचमा'\n",
    " ]\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            #let's remove the suffix\n",
    "            return word[:-len(suffix)]\n",
    "    #here if any word doesn't contain any suffix then return that exact word\n",
    "    return word \n",
    "\n",
    "#function to stem the tokens \n",
    "def stem_words(tokens):\n",
    "    return [custom_stem(word) for word in tokens]\n",
    "\n",
    "#here the unwanted result occurred as our tokenized text was not in the list form\n",
    "#df['cleaned_tokenized_text'] = df['cleaned_tokenized_text'].apply()\n",
    "#applying stemming to the tokenized text column \n",
    "df['stemmed_text'] = df['cleaned_tokenized_text'].apply(stem_words)\n",
    "\n",
    "print(\"Dataframe after stemming:\")\n",
    "print(df[['cleaned_tokenized_text', 'stemmed_text']].head())\n",
    "df.to_csv('C:/Users/Anuz/OneDrive/Desktop/excel work/stemmed_tokenized_cleaned_dataset.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9148c40-c094-4a59-b06f-8585e7b1569b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
