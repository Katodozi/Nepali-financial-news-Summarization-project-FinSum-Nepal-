{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c785e282-a9c4-4868-a618-b90aa994f9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 43047 word embeddings.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the path to your embeddings file\n",
    "embeddings_file_path = r\"C:\\Users\\Anuz\\OneDrive\\Desktop\\excel work\\Embeddings.txt\"\n",
    "\n",
    "# Initialize an empty dictionary to store embeddings\n",
    "embeddings = {}\n",
    "\n",
    "# Read the embeddings from the file\n",
    "with open(embeddings_file_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()  # Split line into word and vector components\n",
    "        word = parts[0]  # The first part is the word\n",
    "        vector = np.array(parts[1:], dtype=float)  # Convert the rest to a NumPy array of floats\n",
    "        embeddings[word] = vector  # Store in the dictionary\n",
    "\n",
    "print(f\"Loaded {len(embeddings)} word embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2517d9aa-8201-43aa-87ee-9e495e3b441d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8639 tokenized sentences.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your tokenized data file\n",
    "tokenized_data_file_path = r\"C:\\Users\\Anuz\\OneDrive\\Desktop\\excel work\\stemmed_tokenized_cleaned_dataset.csv\"\n",
    "\n",
    "# Load the tokenized data from CSV file\n",
    "df = pd.read_csv(tokenized_data_file_path)\n",
    "\n",
    "# Convert string representation of lists back to actual lists\n",
    "tokenized_sentences = df['stemmed_text'].apply(eval).tolist()\n",
    "\n",
    "print(f\"Loaded {len(tokenized_sentences)} tokenized sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f3ad697-1efe-4c36-9ec9-7418a169e2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 43047\n"
     ]
    }
   ],
   "source": [
    "# Create a vocabulary mapping from words to indices\n",
    "word_to_index = {word: idx for idx, word in enumerate(embeddings.keys())}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {len(word_to_index)}\")\n",
    "vocab_dict = word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f419a011-4b7c-4a1a-9c6c-e13486a10c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# max_seq_len = 200  # Maximum length of input sequences\n",
    "# padding_idx = len(word_to_index)  # Use this index for padding (if needed)\n",
    "\n",
    "# sequences_data = []\n",
    "\n",
    "# for tokens in tokenized_sentences:\n",
    "#     # Create a padded sequence initialized with padding index\n",
    "#     padded_sequence = [padding_idx] * max_seq_len\n",
    "    \n",
    "#     # Fill in the indices corresponding to words found in word_to_index\n",
    "#     for i, tok in enumerate(tokens):\n",
    "#         if tok in word_to_index:\n",
    "#             if i < max_seq_len:  # Ensure we do not exceed max_seq_len\n",
    "#                 padded_sequence[max_seq_len - i - 1] = word_to_index[tok]  # Fill from the end\n",
    "\n",
    "#     sequences_data.append(padded_sequence)\n",
    "\n",
    "# # Convert to tensors\n",
    "# sequence_tensors = [torch.tensor(seq, dtype=torch.long) for seq in sequences_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "140f3cc5-aa13-4640-ac46-64f5769693e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_len = 200  # Maximum length of input sequences\n",
    "# padding_idx = 0     # Index for padding (if needed)\n",
    "\n",
    "# # Prepare sequences and labels (you'll need actual target summaries here)\n",
    "# sequences_data = []\n",
    "# labels_data = []  # This should contain your target summaries\n",
    "\n",
    "# for text in df['stemmed_text']:\n",
    "#     tokens = text.split()\n",
    "    \n",
    "#     padded_sequence = [padding_idx]*max_seq_len\n",
    "    \n",
    "#     for i,tok in enumerate(tokens):\n",
    "#         if tok in vocab_dict:\n",
    "#             padded_sequence[max_seq_len-i-1]=vocab_dict[tok]  # Fill from the end\n",
    "            \n",
    "#     sequences_data.append(padded_sequence)\n",
    "\n",
    "# # Convert to tensors\n",
    "# sequence_tensors = [torch.tensor(seq, dtype=torch.long) for seq in sequences_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d68e802d-c56d-4540-96e3-03691f2b6dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SummarizationModel(nn.Module):\n",
    "#     def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "#         super(SummarizationModel, self).__init__()\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, vocab_size)  # Ensure vocab_size matches\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         lstm_out, _ = self.lstm(x)\n",
    "#         out = self.fc(lstm_out[:, -1, :])  # Use output from last time step\n",
    "#         return out\n",
    "\n",
    "# # Define parameters for the model\n",
    "# embedding_dim = 100  # This should match your GloVe embedding size\n",
    "# hidden_dim = 128     # You can adjust this based on your architecture\n",
    "# vocab_size = len(word_to_index) + 1  # Adding 1 for padding index\n",
    "\n",
    "# # Create an instance of the model\n",
    "# model = SummarizationModel(embedding_dim=embedding_dim, hidden_dim=hidden_dim, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6373a4a-6d0b-4278-be67-7b58c6a70934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #defining custom dataset class \n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# class NewsDataset(Dataset):\n",
    "#     def __init__(self, sequences):\n",
    "#         self.sequences = sequences\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.sequences)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.sequences[idx]\n",
    "\n",
    "# # Create dataset and dataloader\n",
    "# dataset = NewsDataset(sequence_tensors)\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0d03b98-dcac-4f91-8e3a-3b56260ef254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #training function for lstm model \n",
    "# def train_model(model, dataloader, epochs=10):\n",
    "#     model.train()  # Set model to training mode\n",
    "    \n",
    "#     criterion = nn.CrossEntropyLoss()  # Loss function\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Optimizer\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         total_loss = 0\n",
    "        \n",
    "#         for inputs in dataloader:\n",
    "#             optimizer.zero_grad()  # Clear previous gradients\n",
    "            \n",
    "#             outputs = model(inputs)  # Get predictions from the model\n",
    "            \n",
    "#             loss = criterion(outputs.view(-1, outputs.size(-1)),targets.view(-1), inputs.view(-1))  # Compute loss\n",
    "            \n",
    "#             loss.backward()  # Backpropagation step\n",
    "            \n",
    "#             optimizer.step()  # Update weights\n",
    "            \n",
    "#             total_loss += loss.item()\n",
    "        \n",
    "#         print(f'Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(dataloader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dd1a957-12db-47d8-b86c-3dd4e60a9d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing the textrank algorithm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_sentence_vector(sentence, embeddings):\n",
    "    tokens = sentence.split()  # Tokenize the sentence into words\n",
    "    vectors = [embeddings[word] for word in tokens if word in embeddings]\n",
    "    \n",
    "    # Return a zero vector if no valid tokens are found\n",
    "    if not vectors:  \n",
    "        return np.zeros(100)  # Return a zero vector of embedding size (100)\n",
    "    \n",
    "    return np.mean(vectors, axis=0)  # Average out the vectors\n",
    "def textrank(sentences, embeddings):\n",
    "    sentence_vectors = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        vector = get_sentence_vector(sentence, embeddings)\n",
    "        # print(f\"Sentence: {sentence}\")  # Print the sentence\n",
    "        # print(f\"Vector shape: {vector.shape}\")  # Print the shape of the vector\n",
    "        sentence_vectors.append(vector)\n",
    "\n",
    "    #Convert list of vectors to a NumPy array\n",
    "    try:\n",
    "        sentence_vectors = np.array(sentence_vectors)\n",
    "    except ValueError as e:\n",
    "        print(\"Error creating NumPy array:\", e)\n",
    "        print(\"Sentence Vectors:\", sentence_vectors)  # Debugging output\n",
    "        return []\n",
    "\n",
    "    # Compute similarity matrix\n",
    "    similarity_matrix = cosine_similarity(sentence_vectors)\n",
    "\n",
    "    # Rank sentences based on scores (sum of similarities)\n",
    "    scores = np.sum(similarity_matrix, axis=1)\n",
    "    ranked_sentences = [sentences[i] for i in np.argsort(scores)[::-1]]  # Sort sentences by score\n",
    "    \n",
    "    return ranked_sentences[:3]  # Return top 3 sentences as summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "979f4dc6-1a05-4223-b576-2178e0a54c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractive Summary: \n",
      "\n",
      "अहिले केन्द्रीय बैंकले नोट निष्कासन गर्छ । खुद्रा सीबीडीसी भएमा त्यसको ‘ब्याकअप’मा बैंक वा केन्द्रीय बैंक हुन्छ ।\n",
      "\n",
      "एक अध्ययनले देखाए अनुसार विश्व अर्थतन्त्रमा ९८ प्रतिशत योगदान रहेका देशका केन्द्रीय बैंकले ‘सेन्ट्रल बैंक डिजिटल करेन्सी’ (सीबीडीसी) विकास गर्न काम गरिरहेका छन् \n"
     ]
    }
   ],
   "source": [
    "test_article = \"\"\"सताब्दियौंदेखि संसारभर कागजी नोट तथा सिक्का (भौतिक मुद्रा) प्रचलनमा छन् । पछिल्लो समय अभौतिक मुद्राको विकासमा विश्वका धेरै मुलुकहरुले काम गरिरहेका छन् ।\n",
    "\n",
    "एक अध्ययनले देखाए अनुसार विश्व अर्थतन्त्रमा ९८ प्रतिशत योगदान रहेका देशका केन्द्रीय बैंकले ‘सेन्ट्रल बैंक डिजिटल करेन्सी’ (सीबीडीसी) विकास गर्न काम गरिरहेका छन् ।\n",
    "\n",
    "सीबीडीसी भनेको भौतिक मुद्रालाई अभौतिक (डिजिटल) रुपमा प्रचलनमा ल्याउने प्रणाली हो । यसको निष्कासन नै अभौतिक रुपमा हुन्छ । भौतिक मुद्रा जस्तै सीबीडीसी पनि विभिन्न किसिमका हुन्छन् । यसको नियन्त्रण केन्द्रीय बैंकबाट नै हुन्छ । यसको कानुनी अधिकार केन्द्रीय बैंकसँग हुन्छ । यसको दायित्व पनि केन्द्रीय बैंकको नै हुन्छ ।\n",
    "\n",
    "अहिले केन्द्रीय बैंकले नोट निष्कासन गर्छ । साथै इ–मनिको रुपमा मोबाइल बैंकिङ, मोबाइल वालेट लगायतको डिजिटल भुक्तानी उपकरण मार्फत कारोबार हुँदै आएको छ । यसको ‘ब्याकअप’ मा बैकहरुसँग नोट नै रहन्छ ।\n",
    "\n",
    "सेन्ट्रल बैंक डिजिटल करेन्सीको पनि ‘केन्द्रीय बैंक’ नै त्यसको ज्ञारेन्टीको रुपमा रहन्छ । खुद्रा सीबीडीसी भएमा त्यसको ‘ब्याकअप’मा बैंक वा केन्द्रीय बैंक हुन्छ । अहिले मोबाइल बैंकिङ, इन्टरनेट बैंकिङ, क्युआर कोडको कारोबार तत्काल भएपनि त्यसको सेटलमेन्ट भने १/२ दिनमा मात्रै हुन्छ ।\n",
    "\n",
    "सीबीडीसी रियल टाइममा सेटलमन्ट हुन्छ । डिजिटल रुपमा मुद्रा निष्कासन गर्ने र त्यसको दायित्व लिनेगरी ‘ब्लकचेन प्रविधि’ मा आधारित रहेर केन्द्रिकृत रुपमा मुद्राको कारोबार गर्न अनुमति प्रदान गरिन्छ भने त्यसलाई सीबीडीसी भनिन्छ ।\n",
    "\n",
    "विश्वव्यापी रुपमा सीबीडीसीको सुरुवात र तीव्र विकास कोभिड सँगसँगै भएको देखिन्छ । त्यसअघि नै ब्लकचेन प्रविधिमा आधारित भएर क्रिप्टोरकरेन्सी तीव्र विकास भइसकेको थियो । त्यसले परम्परागत कागजी मुद्रामाथि चुनौती थपेकै थियो ।\"\"\"\n",
    "\n",
    "# Get extractive summary using TextRank \n",
    "extractive_summary = textrank(test_article.split('।'), embeddings)\n",
    "print(\"Extractive Summary:\", '।'.join(extractive_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d93a336-32df-4bdc-bd79-94640cfdec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def synonym_replacement(sentence):\n",
    "#     synonyms = {\n",
    "#              \"सरकार\": \"सरकारी निकाय\",\n",
    "#     \"बजेट\": \"आर्थिक योजना\",\n",
    "#     \"बेरुजु\": \"अवशेष\",\n",
    "#     \"खर्च\": \"व्यय\",\n",
    "#     \"राजस्व\": \"आम्दानी\",\n",
    "#     \"लेखा परीक्षण\": \"लेखा परिक्षण\",\n",
    "#     \"महालेखा नियन्त्रक\": \"लेखा प्रमुख\",\n",
    "#     \"वित्तीय वर्ष\": \"आर्थिक वर्ष\",\n",
    "#     \"पेस्की\": \"अग्रिम रकम\",\n",
    "#     \"अनुदान\": \"सहयोग राशि\",\n",
    "#     \"ऋण\": \"कर्ज़\",\n",
    "#     \"चक्रवृद्धि ब्याज\": \"चक्र ब्याज\",\n",
    "#     \"निवेश\": \"लगानी\",\n",
    "#     \"वित्तीय विवरण\": \"आर्थिक रिपोर्ट\",\n",
    "#     \"सम्पत्ति\": \"सम्पत्ति साधन\",\n",
    "#     \"दायित्व\": \"जिम्मेवारी\",\n",
    "#     \"लाभांश\": \"मुनाफा वितरण\",\n",
    "#     \"बजार मूल्य\": \"बजार दर\",\n",
    "#     \"शेयर बजार\": \"शेयर व्यापार बजार\",\n",
    "#     \"वित्तीय संस्थान\": \"बैंकिंग संस्था\",\n",
    "#     \"कर प्रणाली\": \"कर संरचना\",\n",
    "#     \"बचत खाता\": \"बचत खाता खाता\",\n",
    "#     \"चालू खाता\": \"संचालन खाता\",\n",
    "#     \"पूँजी बजार\": \"पूँजी व्यापार बजार\",\n",
    "#     \"ऋणपत्र\": \"कर्ज़ पत्रिका\",\n",
    "#     \"मूल्यांकन रिपोर्ट\": \"मूल्यांकन विवरणिका\",\n",
    "#     \"वित्तीय योजना\": \"आर्थिक योजना\",\n",
    "#     \"समग्र घरेलु उत्पादन\": \"कुल घरेलु उत्पादन (GDP)\",\n",
    "#     \"विदेशी मुद्रा भण्डार\": \"अन्तर्राष्ट्रिय मुद्रा भण्डार\",\n",
    "#     \"वित्तीय संकट\": \"आर्थिक संकट\",\n",
    "#     \"स्थायी आय स्रोतहरू\": \"दीर्घकालीन आय स्रोतहरू\",\n",
    "#     \"बिमा प्रिमियम\":  \"बिमा शुल्क\",\n",
    "#     \"भुक्तानी प्रणाली\":  \"भुक्तानी विधि\",\n",
    "#     \"उपभोक्ता मूल्य सूचकांक\":  \"उपभोक्ता मूल्य मापदण्ड\",\n",
    "#     \"मुद्रा स्फीति\":  \"मूल्य वृद्धि\",\n",
    "#     \"ऋण चुकाउने क्षमता\":  \"कर्ज़ चुकाउने क्षमता\",\n",
    "#     \"आर्थिक विकास\":  \"वित्तीय प्रगति\",\n",
    "#     \"बजेट अधिवेशन\":  \"बजेट बैठक\",\n",
    "#     \"वित्तीय अनुगमन\":  \"आर्थिक निगरानी\",\n",
    "#     \"व्यावसायिक योजना\":  \"व्यवसायिक रणनीति\",\n",
    "#     \"सम्पत्ति व्यवस्थापन\":  \"सम्पत्ति नियन्त्रण\",\n",
    "#     \"जोखिम व्यवस्थापन\":  \"जोखिम नियन्त्रण\",\n",
    "#     \"वित्तीय अनुपात\":  \"आर्थिक अनुपात\",\n",
    "#     \"आयकर\":  \"आम्दानी कर\",\n",
    "#     \"सम्पत्ति कर\":  \"सम्पत्ति कराधान\",\n",
    "#     \"बिदेशी लगानी\":  \"बाह्य लगानी\",\n",
    "#     \"व्याज दर\":  \"ब्याजको दर\",\n",
    "#     \"व्यापार घाटा\":  \"व्यापारको कमी\",\n",
    "#     \"आर्थिक नीतिहरू\":  \"वित्तीय नीतिहरू\",\n",
    "#     \"निवेश\": \"लगानी\",\n",
    "# \"संचालन\": \"सञ्चालन\",\n",
    "# \"वित्तीय\": \"आर्थिक\",\n",
    "# \"उपाय\": \"उपाय\",\n",
    "# \"स्रोत\": \"साधन\",\n",
    "# \"समीक्षा\": \"परिक्षण\",\n",
    "# \"सञ्चय\": \"बचत\",\n",
    "# \"सहयोग\": \"समर्थन\",\n",
    "# \"आवश्यकता\": \"आवश्यकता\",\n",
    "# \"सम्भावना\": \"अवसर\",\n",
    "# \"दिशा\": \"मार्गदर्शन\",\n",
    "# \"बिक्री\": \"बिक्री\",\n",
    "# \"प्रवृत्ति\": \"प्रवृत्ति\",\n",
    "# \"संरचना\": \"ढाँचा\",\n",
    "# \"अवधि\": \"समय सीमा\",\n",
    "# \"प्रभाव\": \"असर\",\n",
    "# \"सञ्चालन खर्च\": \"सञ्चालन व्यय\",\n",
    "# \"आर्थिक नीति\": \"वित्तीय नीति\",\n",
    "# \"बजार विश्लेषण\": \"बजार अध्ययन\",\n",
    "# \"स्रोतहरू\": \"साधनहरू\",\n",
    "# \"अर्थशास्त्र\": \"अर्थ विज्ञान\",\n",
    "# \"प्रतिस्पर्धा\": \"प्रतिस्पर्धा\",\n",
    "# \"लाभप्रदता\": \"मुनाफा बनाउने क्षमता\",\n",
    "# \"सम्पर्क\": \"सम्बन्ध\",\n",
    "# \"बिकास\": \"प्रगति\",\n",
    "# \"परियोजना\": \"योजना\",\n",
    "# \"संलग्नता\": \"संलग्नता\",\n",
    "# \"सामग्री\": \"सामग्रीहरू\",\n",
    "# \"पारदर्शिता\": \"स्पष्टता\",\n",
    "# \"विकासशील देशहरू\": \"विकासशील राष्ट्रहरू\",\n",
    "# \"आधारभूत संरचना\": \"आधारभूत ढाँचा\",\n",
    "# \"साझेदारी\": \"सहकार्य\",\n",
    "# \"सूचना प्रविधि\": \"सूचना टेक्नोलोजी\",\n",
    "# \"व्यापारिक गतिविधि\": \"व्यापार क्रियाकलाप\",\n",
    "# \"आर्थिक गतिविधि\": \"वित्तीय क्रियाकलाप\",\n",
    "# \"पुनर्निर्माण गर्नुपर्छ\": \"सुधार गर्नुपर्छ\",\n",
    "# \"उपाययोजना\": \"योजना बनाउनुपर्छ\",\n",
    "# \"अभियान\": \"कार्यक्रम\",\n",
    "# \"प्रतिबद्धता\": \"वचनबद्धता\",\n",
    "# \"स्रोतको उपयोग\": \"साधनको प्रयोग\",\n",
    "# \"लाभदायक\": \"फाइदा दिने\",\n",
    "# \"बिकासशील\": \"प्रगतिशील\",\n",
    "# \"अनुसन्धान\": \"अध्ययन\",\n",
    "# \"प्रतिस्पर्धात्मक लाभ\": \"प्रतिस्पर्धात्मक फाइदा\",\n",
    "# \"आधारभूत तत्वहरू\": \"मूल तत्वहरू\",\n",
    "# \"व्यापारिक रणनीति\": \"व्यापार योजना\",\n",
    "# \"उपभोक्ता व्यवहार\": \"ग्राहक व्यवहार\"\n",
    "#     }\n",
    "\n",
    "#     for word, synonym in synonyms.items():\n",
    "#         sentence = sentence.replace(word, synonym)\n",
    "\n",
    "#     return sentence\n",
    "\n",
    "# # Assuming extractive_summary is a list of sentences\n",
    "# original_sentences = extractive_summary  # This should be your list of sentences\n",
    "# modified_sentences = [synonym_replacement(sentence) for sentence in original_sentences]\n",
    "\n",
    "# # If you want to join them back into a single string\n",
    "# modified_text = ' '.join(modified_sentences)\n",
    "# print(modified_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
